---
title: "Clustering and Dimensionality Reduction"
author: "Biagio Alessandrello"
date: "2024-08-12"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rpart)
library(rpart.plot)
library(dplyr)
library(knitr)
```

## Clustering and dimensionality reduction  

The data in [wine.csv](../data/wine.csv) contains information on 11 chemical properties of 6500 different bottles of _vinho verde_ wine from northern Portugal.  In addition, two other variables about each wine are recorded:
- whether the wine is red or white  
- the quality of the wine, as judged on a 1-10 scale by a panel of certified wine snobs.  

Run PCA, tSNE, and any clustering algorithm of your choice on the 11 chemical properties (or suitable transformations thereof) and summarize your results.  Which dimensionality reduction technique makes the most sense to you for this data?  Convince yourself (and me) that your chosen approach is easily capable of distinguishing the reds from the whites, using only the "unsupervised" information contained in the data on chemical properties.  Does your unsupervised technique also seem capable of distinguishing the higher from the lower quality wines?  Present appropriate numerical and/or visual evidence to support your conclusions.  

To clarify: I'm not asking you to run a supervised learning algorithms.  Rather, I'm asking you to see whether the differences in the labels (red/white and quality score) emerge naturally from applying an unsupervised technique to the chemical properties.  This should be straightforward to assess using plots.  

### PCA

	•	Standard deviation: This is the square root of the eigenvalue associated with PC1. It measures the amount of variability in the data captured by PC1. A higher standard deviation means PC1 explains more variance.
	
	•	Proportion of Variance: This indicates that PC1 explains % of the total variance in the data. This is a substantial portion, meaning that PC1 captures most of the variability in your dataset.
	
	•	Cumulative Proportion: Since there’s only one principal component considered here, the cumulative proportion is the same as the proportion of variance explained by PC1. This means that by considering just the first principal component, you are accounting for % of the total variance in the original data.

```{r, echo=FALSE}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(caret)
library(Rtsne)
library(cluster)
library(factoextra)

# Load the data
wine_data <- read.csv("wine.csv")

# View the structure of the data
str(wine_data)
```

```{r, echo=FALSE}
# Check for missing values
sum(is.na(wine_data))

# Standardize the chemical properties for PCA and t-SNE
wine_data_scaled <- scale(wine_data[, 1:11])

# Add wine type (red/white) and quality back to the standardized data for plotting purposes
wine_data_scaled <- as.data.frame(wine_data_scaled)
wine_data_scaled$color <- wine_data$color
wine_data_scaled$quality <- wine_data$quality
```

```{r, echo=FALSE}
# Perform PCA
pca_result <- prcomp(wine_data_scaled[, 1:11], scale. = TRUE)

# Summary of PCA
summary(pca_result)

# Plot PCA
pca_data <- as.data.frame(pca_result$x)
pca_data$color <- wine_data$color
pca_data$quality <- wine_data$quality

# Color
ggplot(pca_data, aes(x = PC1, y = PC2, color = color)) +
  geom_point(alpha = 0.7) +
  labs(title = "PCA of Wine Data - Color",
       x = "Principal Component 1",
       y = "Principal Component 2")

# Quality
ggplot(pca_data, aes(x = PC1, y = PC2, color = quality)) +
  geom_point(alpha = 0.7) +
  labs(title = "PCA of Wine Data - Quality",
       x = "Principal Component 1",
       y = "Principal Component 2")
```

### tSNE

```{r}
# Remove duplicate rows
wine_data_unique <- wine_data[!duplicated(wine_data[, 1:11]), ]

# Standardize the chemical properties for PCA and t-SNE
wine_data_scaled_unique <- scale(wine_data_unique[, 1:11])

# Add wine type (red/white) and quality back to the standardized data for plotting purposes
wine_data_scaled_unique <- as.data.frame(wine_data_scaled_unique)
wine_data_scaled_unique$color <- wine_data_unique$color
wine_data_scaled_unique$quality <- wine_data_unique$quality

# Set seed for reproducibility
set.seed(123)

# Perform t-SNE on the unique data
tsne_result <- Rtsne(wine_data_scaled_unique[, 1:11], dims = 2, perplexity = 50, verbose = TRUE, max_iter = 500)

# Prepare t-SNE data for plotting
tsne_data <- as.data.frame(tsne_result$Y)
colnames(tsne_data) <- c("Dim1", "Dim2")
tsne_data$color <- wine_data_scaled_unique$color
tsne_data$quality <- wine_data_scaled_unique$quality

# Plot t-SNE Color
ggplot(tsne_data, aes(x = Dim1, y = Dim2, color = color)) +
  geom_point(alpha = 0.7) +
  labs(title = "t-SNE of Wine Data",
       x = "Dimension 1",
       y = "Dimension 2")

# Plot t-SNE Quality
ggplot(tsne_data, aes(x = Dim1, y = Dim2, color = quality)) +
  geom_point(alpha = 0.7) +
  labs(title = "t-SNE of Wine Data",
       x = "Dimension 1",
       y = "Dimension 2")
```
### Visual Inspection

Looking at both PCA and t-SNE, They are both doing a great job at distinguishing between the colors of wine but you can tell that t-SNE was able to distinguish the two wine colors more effectively than PCA. On the quality side of things it is more difficult to tell as the qualities seem to be more spread out through both plots.

### Cluster Seperation


### Explained Variance (PCA Only)

It takes PCA raching 4 components before it reaches and acceptable capture of the structure of data.

### Silhouette Score (Cluster Performance)

```{r}
library(cluster)

# Clustering with PCA results
kmeans_pca <- kmeans(pca_data[, 1:2], centers = 2)  # Assuming 2 clusters for red and white
silhouette_pca <- silhouette(kmeans_pca$cluster, dist(pca_data[, 1:2]))
avg_silhouette_pca <- mean(silhouette_pca[, 3])  # Average silhouette score for PCA

# Clustering with t-SNE results
kmeans_tsne <- kmeans(tsne_data[, 1:2], centers = 2)  # Assuming 2 clusters for red and white
silhouette_tsne <- silhouette(kmeans_tsne$cluster, dist(tsne_data[, 1:2]))
avg_silhouette_tsne <- mean(silhouette_tsne[, 3])  # Average silhouette score for t-SNE

# Compare the scores
cat("Average Silhouette Score for PCA: ", avg_silhouette_pca, "\n")
cat("Average Silhouette Score for t-SNE: ", avg_silhouette_tsne, "\n")
```

PCA is showing a higher Average Silhouette making it a more effective model by this metric

### Visual Clarity in Differentiating Labels

After looking at color seperation, it looks like both the PCA and the t-SNE have done a great job at seperating out visually for the color of wine. In the quality part they are both not doing so great.

### Neighborhood Preservation (t-SNE Specific)

```{r}
# Perform K-means clustering
set.seed(123)
kmeans_result <- kmeans(wine_data_scaled[, 1:11], centers = 2)

# Add cluster assignments to the data
wine_data_scaled$cluster <- as.factor(kmeans_result$cluster)

# Plot clusters and compare with actual color
ggplot(wine_data_scaled, aes(x = pca_result$x[, 1], y = pca_result$x[, 2], color = cluster)) +
  geom_point(alpha = 0.7) +
  labs(title = "K-means Clustering on Wine Data",
       x = "Principal Component 1",
       y = "Principal Component 2")
```

