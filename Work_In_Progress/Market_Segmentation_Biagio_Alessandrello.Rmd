---
title: "Market Segmentation"
author: "Biagio Alessandrello"
date: "2024-08-13"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rpart)
library(rpart.plot)
library(dplyr)
library(knitr)
library(caret)
library(Rtsne)
library(cluster)
library(factoextra)
library(ggplot2)
```

## Market segmentation

Consider the data in [social_marketing.csv](../data/social_marketing.csv).  This was data collected in the course of a market-research study using followers of the Twitter account of a large consumer brand that shall remain nameless---let's call it "NutrientH20" just to have a label.  The goal here was for NutrientH20 to understand its social-media audience a little bit better, so that it could hone its messaging a little more sharply.

A bit of background on the data collection: the advertising firm who runs NutrientH20's online-advertising campaigns took a sample of the brand's Twitter followers.  They collected every Twitter post ("tweet") by each of those followers over a seven-day period in June 2014.  Every post was examined by a human annotator contracted through [Amazon's Mechanical Turk](https://www.mturk.com/mturk/welcome) service.  Each tweet was categorized based on its content using a pre-specified scheme of 36 different categories, each representing a broad area of interest (e.g. politics, sports, family, etc.)  Annotators were allowed to classify a post as belonging to more than one category.  For example, a hypothetical post such as "I'm really excited to see grandpa go wreck shop in his geriatic soccer league this Sunday!" might be categorized as both "family" and "sports."  You get the picture.

Each row of [social_marketing.csv](../data/social_marketing.csv) represents one user, labeled by a random (anonymous, unique) 9-digit alphanumeric code.  Each column represents an interest, which are labeled along the top of the data file.  The entries are the number of posts by a given user that fell into the given category.  Two interests of note here are "spam" (i.e. unsolicited advertising) and "adult" (posts that are pornographic, salacious, or explicitly sexual).  There are a lot of spam and pornography ["bots" on Twitter](http://mashable.com/2013/11/08/twitter-spambots/); while these have been filtered out of the data set to some extent, there will certainly be some that slip through.  There's also an "uncategorized" label.  Annotators were told to use this sparingly, but it's there to capture posts that don't fit at all into any of the listed interest categories.  (A lot of annotators may used the "chatter" category for this as well.)  Keep in mind as you examine the data that you cannot expect perfect annotations of all posts.  Some annotators might have simply been asleep at the wheel some, or even all, of the time!  Thus there is some inevitable error and noisiness in the annotation process.

Your task to is analyze this data as you see fit, and to prepare a concise report for NutrientH20 that identifies any interesting market segments that appear to stand out in their social-media audience.  You have complete freedom in deciding how to pre-process the data and how to define "market segment." (Is it a group of correlated interests?  A cluster?  A latent factor?  Etc.)  Just use the data to come up with some interesting, well-supported insights about the audience, and be clear about what you did.

```{r}
social <- read.csv("social_marketing.csv", header = TRUE)

str(social)
head(social)
```

```{r}
# Missing Data 

# Check for missing values
sum(is.na(social))

# If there are missing values, we can either fill them with zeroes (assuming missing means no activity) or remove those rows
social[is.na(social)] <- 0
```

### Exploratory Data Analysis

```{r}
summary(social)

interest_stats <- social %>%
  select(-1) %>%
  summarise_all(list(mean = mean, median = median, sd = sd, max = max, min = min))

print(interest_stats)
```

# Correlation Analysis

```{r}
# Compute correlation matrix
cor_matrix <- cor(social[,-1])  # Exclude the ID column

# Visualize the correlation matrix
library(corrplot)
corrplot(cor_matrix, method = "circle", type = "upper", tl.col = "black", tl.cex = 0.8)
```

### Dimensionality Reduction

```{r}
# Standardize the data (excluding the ID column)
social_scaled <- scale(social[,-1])

# Perform PCA
pca_result <- prcomp(social_scaled, scale. = TRUE)

# Plot the variance explained by each principal component
fviz_eig(pca_result)

# Biplot of the first two principal components
fviz_pca_biplot(pca_result, geom.ind = "point", pointshape = 21, pointsize = 2, 
                fill.ind = "skyblue", col.ind = "black", repel = TRUE) +
  labs(title = "PCA Biplot", x = "PC1", y = "PC2")
```


### H Cluster

```{r}
summary(social)

# Center and scale the data
X = social[,2:37]
X = scale(X, center=TRUE, scale=TRUE)

# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")

# First form a pairwise distance matrix
distance_between_variables = dist(X)

# Now run hierarchical clustering
h1 = hclust(distance_between_variables, method='complete')

# Cut the tree into 10 clusters
cluster1 = cutree(h1, k=5)
summary(factor(cluster1))

# Examine the cluster members
which(cluster1 == 4)

# Plot the dendrogram
plot(h1, cex=0.3)

# Now run hierarchical clustering
h2 = hclust(distance_between_variables, method='single')
plot(h2, cex=0.3)
```

### PCA


